[
  {
    "user": "HumanAIGC",
    "name": "animate-anyone",
    "readme": "<h1>animate_anyone.github.io</h1>\n<p>Project Page for Animate Anyone</p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "animate-anyone-2",
    "readme": "<p>Project Page for Animate Anyone 2 </p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "AnimateAnyone",
    "readme": "<h1>AnimateAnyone</h1>\n<p>Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation</p>\n<p><a href=\"https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=Arz3iGUAAAAJ&gmla=AJsN-F72u4R_vwVl2Jc0Sy_qIYuSwExx8ilpfrd-w5Yfi5FYFP_WhbJtHbAK_c5w-3KNBgTRjWiTvEFLtJSV5ryd1JuNVQdMVDMuSJS5dfn7NWbZQQpGGyyxlrfoq6cv6S_23QTSUWWY\">Li Hu</a>, \n<a href=\"https://scholar.google.com/citations?user=cze1sXQAAAAJ&hl=en\">Xin Gao</a>, \n<a href=\"https://scholar.google.com/citations?user=QTgxKmkAAAAJ&hl=zh-CN\">Peng Zhang</a>,\n<a href=\"https://dblp.org/pid/69/476-9.html\">Ke Sun</a>, \n<a href=\"https://dblp.org/pid/11/4046.html\">Bang Zhang</a>,\n<a href=\"https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN\">Liefeng Bo</a></p>\n<p><a href='https://humanaigc.github.io/animate-anyone/'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/pdf/2311.17117.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href=\"https://www.youtube.com/watch?v=8PCn5hLKNu4\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<p><img src=\"docs/video_t1.png\" alt=\"Teaser Image\" title=\"Teaser\"></p>\n<h2>Citation</h2>\n<pre><code>@article{hu2023animateanyone,\n  title={Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation},\n  author={Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},\n  journal={arXiv preprint arXiv:2311.17117},\n  website={https://humanaigc.github.io/animate-anyone/},\n  year={2023}\n}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "Cloth2Tex",
    "readme": "<p align=\"center\">\n\n  <h1 align=\"center\">Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On</h1>\n  <h2 align=\"center\">3DV 2024</h2>\n  <div align=\"center\">\n    <img src=\"./imgs/teaser.png\" alt=\"Logo\" width=\"100%\">\n  </div>\n  \n  <div class=\"is-size-5 publication-authors\">\n    <span class=\"author-block\">\n        <a href=\"\">Daiheng Gao</a><sup>1</sup>,</span>\n    <span class=\"author-block\">\n        <a href=\"\">Xu Chen</a><sup>2,3</sup>,</span>\n    <span class=\"author-block\">\n        <a href=\"\">Xindi Zhang</a><sup>1</sup>,\n    </span>\n    <span class=\"author-block\">\n        <a href=\"\">Qi Wang</a><sup>1</sup>,\n    </span>\n    <span class=\"author-block\">\n        <a href=\"\">Ke Sun</a><sup>1</sup>,\n    </span>\n    <span class=\"author-block\">\n        <a href=\"\">Bang Zhang</a><sup>1</sup>,\n    </span>\n    <span class=\"author-block\">\n        <a href=\"\">Liefeng Bo</a><sup>1</sup>,\n    </span>\n    <span class=\"author-block\">\n        <a href=\"\">Qixing Huang</a><sup>4</sup>,\n    </span>\n    </div>\n    <div class=\"is-size-5 publication-authors\">\n    <span class=\"author-block\"><sup>1</sup>Alibaba XR Lab,</span>\n    <span class=\"author-block\"><sup>2</sup>ETH Zurich, Department of Computer Science,</span>\n    <span class=\"author-block\"><sup>3</sup>Max Planck Institute for Intelligent Systems,</span>\n    <span class=\"author-block\"><sup>4</sup>The University of Texas at Austin</span>\n    </div>\n    </div>\n\n\n  <p align=\"center\">\n  <br>\n<br></br>\n    <a href='https://arxiv.org/abs/2308.04288'>\n      <img src='https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&logo=arXiv&logoColor=green' alt='Paper PDF'>\n    </a>\n    <a href='https://tomguluson92.github.io/projects/cloth2tex/' style='padding-left: 0.5rem;'>\n      <img src='https://img.shields.io/badge/Cloth2Tex-Page-blue?style=for-the-badge&logo=Google%20chrome&logoColor=blue' alt='Project Page'>\n    <a href=\"https://www.youtube.com/watch?v=RFMNKe6supE\"><img alt=\"youtube views\" title=\"Subscribe to my YouTube channel\" src=\"https://img.shields.io/youtube/views/hZd6AYin2DE?logo=youtube&labelColor=ce4630&style=for-the-badge\"/></a>\n  </p>\n</p>\n  </p>\n</p>\n\n<br />\n\n<hr>\n<h2>1. Installation</h2>\n<p>Our enviroment is <em>python3.8, pytorch1.13, cuda11.7</em>, you can change the following instructions that suitable for your settings.</p>\n<pre><code class=\"language-shell\">    sudo apt-get update -y\n    sudo apt-get install libgl1\n    sudo apt-get install libboost-dev\n</code></pre>\n<ul>\n<li><strong>pytorch3d</strong> <a href=\"https://pytorch3d.org/\">code</a></li>\n<li><strong>psbody-mesh</strong> <a href=\"https://github.com/MPI-IS/mesh\">code</a></li>\n<li><strong>Kaolin</strong> <a href=\"https://github.com/NVIDIAGameWorks/kaolin\">code</a></li>\n<li><strong>torch_geometric</strong> <a href=\"https://data.pyg.org/whl/\">code</a></li>\n</ul>\n<pre><code>pip install torch_geometric\npip install pyg_lib-0.3.0+pt113cu117-cp38-cp38-linux_x86_64.whl\npip install torch_cluster-1.6.1+pt113cu117-cp38-cp38-linux_x86_64.whl\npip install torch_scatter-2.1.1+pt113cu117-cp38-cp38-linux_x86_64.whl\npip install torch_sparse-0.6.15+pt113cu117-cp38-cp38-linux_x86_64.whl\n</code></pre>\n<hr>\n<h2>2. Architecture</h2>\n<p>Cloth2Tex is composed of two phase: (1) <strong>Coarse texture generation</strong> and (2) <strong>Fine texture completion</strong>. Where Phase I is to determine the 3D garment shape and coarse texture. We do this by registering our parametric garment meshes onto catalog images using a neural mesh renderer. The pipeline’s Then Phase II is to recover fine textures from the coarse estimate of Phase I. We use image translation networks trained on large-scale data synthesized by pre-trained latent diffusion models.</p>\n<p>We only made <strong>Phase I</strong> publicly available for now.</p>\n<p><img src=\"imgs/method.png\" alt=\"\"></p>\n<hr>\n<h2>3. Inference</h2>\n<h3>Phase I (w/o automatic scaling mechanism)</h3>\n<pre><code class=\"language-shell\">python phase1_inference.py --g 1_wy --s 1.2 --d &quot;20231017_wy&quot; --steps_one 501 --steps_two 1001\n</code></pre>\n<p>The optimized results are saved in <code>experiments/20231017_wy</code>, <code>x_texture_uv_1000.jpg</code> is the final UV texture.</p>\n<p>Users can check it with <strong>Blender</strong>, remember you should only reserve one material, and remove other redundant materials for textured mesh.</p>\n<p><img src=\"imgs/phase1_demo.png\" alt=\"img\"></p>\n<h4>(a) reference scale coefficient</h4>\n<p>The noteworthy thing here is that we are not make automatic scaling mechanism code publicly available, if you need it, you could self-implement it or manually adjust the <code>--s</code> (scale).</p>\n<p>Default coefficient for test images:</p>\n<pre><code class=\"language-python\">per_scale_dict = {&quot;1_wy&quot;: 1.1,\n                  &quot;2_Polo&quot;: 0.8, # default 0.8\n                  &quot;3_Tshirt&quot;: 0.9, # default 0.7\n                  &quot;4_shorts&quot;: 0.75, # # default 0.7\n                  &quot;5_trousers&quot;: 0.75,\n                  &quot;6_zipup&quot;: 1.1,\n                  &quot;7_windcoat&quot;: 0.65,\n                  &quot;9_jacket&quot;: 1.0,\n                  &quot;11_skirt&quot;: 1.0} \n</code></pre>\n<h4>(b) The landmark detector</h4>\n<p>We are not going to release the 2D landmark detector. If you need an accurate 2D landmarks in accordance with <strong>Cloth2Tex</strong>, you can annotate it manually or train a simple 2D cloth landmark detector with the same definition from <strong>Cloth2Tex</strong>.</p>\n<h3>Phase II (Inpainting/Completion Network)</h3>\n<p>We are applying for the open-source of Phase II, we will update once approval procedure has finished.</p>\n<hr>\n<h2>4. Demo</h2>\n<h3>Real world 3D Try-On</h3>\n<p><img src=\"imgs/tryon1.png\" alt=\"\">\n<img src=\"imgs/tryon2.png\" alt=\"\"></p>\n<p>Please check cloth2tex web page for animated visual results: <a href=\"https://tomguluson92.github.io/projects/cloth2tex/\">cloth2tex</a> or check our youtube video <a href=\"https://www.youtube.com/watch?v=RFMNKe6supE\">youtube</a>.</p>\n<hr>\n<h2>5. Citation</h2>\n<pre><code class=\"language-bibtex\">@article{gao2023cloth2tex,\n  title={Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On},\n  author={Gao, Daiheng and Chen, Xu and Zhang, Xindi and Wang, Qi and Sun, Ke and Zhang, Bang and Bo, Liefeng and Huang, Qixing},\n  journal={arXiv preprint arXiv:2308.04288},\n  year={2023}\n}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "EMO",
    "readme": "<h1>EMO</h1>\n<p>Emote Portrait Alive: Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</p>\n<p>Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo,</p>\n<p>Institute for Intelligent Computing, Alibaba Group</p>\n<p><strong> at European Conference on Computer Vision (ECCV) 2024 </strong></p>\n<p><a href='https://humanaigc.github.io/emote-portrait-alive/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>\n<a href='https://arxiv.org/abs/2402.17485'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>\n<a href=\"https://youtu.be/VlJ71kzcn9Y\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<h2>Citation</h2>\n<pre><code>@misc{tian2024emo,\n      title={EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions}, \n      author={Linrui Tian and Qi Wang and Bang Zhang and Liefeng Bo},\n      year={2024},\n      eprint={2402.17485},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "emote-portrait-alive",
    "readme": "<p><a href=\"https://humanaigc.github.io/emote-portrait-alive/\">https://humanaigc.github.io/emote-portrait-alive/</a></p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "emote-portrait-alive-2",
    "readme": "<p>This the webpage for EMO2.\n<a href=\"https://humanaigc.github.io/emote-portrait-alive-2/\">https://humanaigc.github.io/emote-portrait-alive-2/</a></p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "lite-avatar",
    "readme": "<h1>LiteAvatar</h1>\n<p>We introduce a audio2face model for realtime 2D chat avatar, which can run in 30fps on only CPU devices without GPU acceleration.</p>\n<h2>Pipeline</h2>\n<ul>\n<li>An efficient ASR model from <a href=\"https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\">modelsope</a> for audio feature extraction.</li>\n<li>A mouth parameter prediction model given audio feature inputs for voice synchronized mouth movement generation.</li>\n<li>A lightweight 2D face generator model for mouth movement rendering, which can also be deployed on mobile devices realizing realtime inference.</li>\n</ul>\n<h2>Data Preparation</h2>\n<p>Get sample avatar data located in <code>./data/sample_data.zip</code> and extract to you path</p>\n<h2>Installation</h2>\n<p>We recommend a python version = 3.10 and cuda version = 11.8. Then build environment as follows:</p>\n<pre><code class=\"language-shell\">pip install -r requirements.txt\n</code></pre>\n<h2>Inference</h2>\n<pre><code>python lite_avatar.py --data_dir /path/to/sample_data --audio_file /path/to/audio.wav --result_dir /path/to/result\n</code></pre>\n<p>The mp4 video result will be saved in the result_dir.</p>\n<h2>Interactive demo</h2>\n<p>The realtime interactive video chat demo powered by our LiteAvatar algorithm is available at <a href=\"https://github.com/HumanAIGC-Engineering/OpenAvatarChat\">OpenAvatarChat</a></p>\n<h2>Acknowledgement</h2>\n<p>We are grateful for the following open-source projects that we used in this project:</p>\n<ul>\n<li><a href=\"https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\">Paraformer</a>\n and <a href=\"https://github.com/modelscope/FunASR\">FunASR</a> for audio feature extraction.</li>\n</ul>\n<h2>Citation</h2>\n<p>If you find this project useful, please ⭐️ star the repository and cite our related paper:</p>\n<pre><code>@inproceedings{ZhuangQZZT22,\n  author       = {Wenlin Zhuang and Jinwei Qi and Peng Zhang and Bang Zhang and Ping Tan},\n  title        = {Text/Speech-Driven Full-Body Animation},\n  booktitle    = {Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, {IJCAI}},\n  pages        = {5956--5959},\n  year         = {2022}\n}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "MaTe3D",
    "readme": "<h1>MaTe3D</h1>\n<p>MaTe3D: Mask-guided Text-based 3D-aware Portrait Editing</p>\n<p><a href=\"https://montaellis.github.io/\">Kangneng Zhou</a>, <a href=\"https://tomguluson92.github.io/\">Daiheng Gao</a>, <a href=\"https://xuanwangvc.github.io/\">Xuan Wang</a>, <a href=\"https://scholar.google.com/citations?user=gBkYZeMAAAAJ\">Jie Zhang</a>, <a href=\"https://scholar.google.com/citations?user=QTgxKmkAAAAJ&hl=zh-CN\">Peng Zhang</a>, <a href=\"https://dblp.org/pid/308/0824.html\">Xusen Sun</a>, <a href=\"https://scholar.google.com/citations?user=qkJD6c0AAAAJ\">Longhao Zhang</a>, <a href=\"https://www.shiqiyang.xyz/\">Shiqi Yang</a>, <a href=\"https://dblp.org/pid/11/4046.html\">Bang Zhang</a>, <a href=\"https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN\">Liefeng Bo</a>, <a href=\"https://scholar.google.es/citations?user=6CsB8k0AAAAJ\">Yaxing Wang</a></p>\n<p><a href='https://humanaigc.github.io/MaTe3D/'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/abs/2312.06947'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href=\"https://www.youtube.com/watch?v=6R5MXhz14R8\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<p><img src=\"docs/teaser.png\" alt=\"Teaser Image\" title=\"Teaser\"></p>\n<h2>Citation</h2>\n<pre><code>@article{zhou2023mate3d,\n  title     = {MaTe3D: Mask-guided Text-based 3D-aware Portrait Editing},\n  author    = {Kangneng Zhou, Daiheng Gao, Xuan Wang, Jie Zhang, Peng Zhang, Xusen Sun, Longhao Zhang, Shiqi Yang, Bang Zhang, Liefeng Bo, Yaxing Wang},\n  journal   = {arXiv preprint arXiv:2312.06947},\n  website   = {https://humanaigc.github.io/MaTe3D/},\n  year      = {2023}}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "outfit-anyone",
    "readme": "<h1>Project Page for Outfit Anyone</h1>\n<p><a href=\"https://humanaigc.github.io/outfit-anyone/\">https://humanaigc.github.io/outfit-anyone/</a></p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "OutfitAnyone",
    "readme": "<h1>Outfit Anyone</h1>\n<p>OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any Person</p>\n<p>Institute for Intelligent Computing, Alibaba Group</p>\n<p><a href=\"https://dblp.org/pid/69/476-9.html\">Ke Sun</a>, \nJian Cao,\n<a href=\"https://scholar.google.com/citations?user=MVeptcsAAAAJ&hl=en&oi=sra\">Qi Wang</a>,\nLinrui Tian,\nXindi Zhang,\nLian Zhuo,\n<a href=\"https://dblp.org/pid/11/4046.html\">Bang Zhang</a>,\n<a href=\"https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN\">Liefeng Bo</a>,\n<a href=\"https://tomguluson92.github.io/\">Daiheng Gao</a></p>\n<p><a href='[https://humanaigc.github.io/animate-anyone/](https://humanaigc.github.io/outfit-anyone/)'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/pdf/2407.16224.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a></p>\n<p><strong><span style=\"color:red\">\nNews</span></strong>: Now you can try out OutfitAnyone v0.9 on <a href = \"https://modelscope.cn/studios/DAMOXR/OutfitAnyone\">ModelScope 魔搭</a> ( for Chinese users ) . Enjoy it and have fun!</p>\n<p><font color='red'><strong>News:</strong></font>\nThe experience demo of OutfitAnyone v0.9 is now available on <a href = 'https://huggingface.co/spaces/HumanAIGC/OutfitAnyone' >Hugging Face</a>.\n<a href=\"https://huggingface.co/spaces/HumanAIGC/OutfitAnyone\"  style='padding-left: 0.5rem;'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OutfitAnyone-orange'></a></p>\n<p>To prevent the malicious use of personal photos, we have currently limited our feature to only allow the upload of clothing images. All models displayed are pre-set and generated using our AI model to ensure safety and privacy.</p>\n<p>Feel free to enjoy OutfitAnyone and share your interesting results with us. If you have any questions or suggestions, don&#39;t hesitate to leave a message in the issues section.</p>\n<p><font color='red'><strong>Note: Please don&#39;t forget to give us a star if you like this project. Thanks!</strong></font> :stuck_out_tongue_winking_eye:</p>\n<hr>\n<p><a href='https://humanaigc.github.io/outfit-anyone/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href=\"https://youtu.be/-KmoKYLbN4c\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<p>OutfitAnyone+AnimateAnyone: <a href=\"https://youtu.be/jnNHcLdoxNk\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<p><img src=\"docs/1.gif\" alt=\"oufit animie\"><img src=\"docs/2.gif\" alt=\"oufit animie\"></p>\n<hr>\n<h2>Citation</h2>\n<pre><code>@article{sun2024outfitanyone,\n  title={OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any Person},\n  author={Sun, Ke and Cao, Jian and Wang, Qi and Tian, Linrui and Zhang, Xindi and Zhuo, Lian and Zhang, Bang and Bo, Liefeng and Zhou, Wenbo and Zhang, Weiming and Daiheng Gao},\n  journal={arXiv preprint arXiv:2407.16224},\n  year={2024}\n}\n</code></pre>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "vivid-talk",
    "readme": "<h1>vivid-talk</h1>\n<p>Project Page for VividTalk</p>\n"
  },
  {
    "user": "HumanAIGC",
    "name": "VividTalk",
    "readme": "<h1>VividTalk</h1>\n<p>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</p>\n<p><a href=\"https://dblp.org/pid/308/0824\">Xusen Sun</a>, <a href=\"https://scholar.google.com/citations?user=qkJD6c0AAAAJ&hl=zh-CN\">Longhao Zhang</a>, <a href=\"http://zhuhao.cc/home/\">Hao Zhu</a>, <a href=\"https://scholar.google.com/citations?user=QTgxKmkAAAAJ&hl=zh-CN\">Peng Zhang</a>, <a href=\"https://dblp.org/pid/11/4046.html\">Bang Zhang</a>, <a href=\"https://dblp.org/pid/290/1747\">Xinya Ji</a>, <a href=\"https://scholar.google.com.hk/citations?user=y1vvxWYAAAAJ&hl=zh-CN\">Kangneng Zhou</a>, <a href=\"https://tomguluson92.github.io/\">Daiheng Gao</a>, <a href=\"https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN\">Liefeng Bo</a>, <a href=\"https://scholar.google.com/citations?user=8hZIngIAAAAJ&hl=zh-CN&oi=ao\">Xun Cao</a></p>\n<p><a href='https://humanaigc.github.io/vivid-talk/'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/pdf/2312.01841.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href=\"https://www.youtube.com/watch?v=lJVzt7JCe_4\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"YouTube\"></a></p>\n<p><img src=\"docs/teaser.png\" alt=\"Teaser Image\" title=\"Teaser\"></p>\n<h2>Citation</h2>\n<pre><code>@article{sun2023vividtalk,\n  title     = {VividTalk: One-Shot Audio-Driven Talking Head Generation Based 3D Hybrid Prior},\n  author    = {Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, Xun Cao},\n  journal   = {arXiv preprint arXiv:2312.01841},\n  website   = {https://humanaigc.github.io/vivid-talk/},\n  year      = {2023},\n</code></pre>\n"
  }
]